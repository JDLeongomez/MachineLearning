---
title: "Week 2: The `caret` package"
subtitle: "**Lectures^[All lectures by [Jeffrey Leek](http://jtleek.com/) (John Hopkins Bloomberg Scool of Public Health).] :** 1. The `caret` package, 2. Data slicing, 3. Training options, 4. Plotting predictions, 5. Basic preprocessing, 6. Covariate creation, 7. Preprocessing with PCA, 8. Predicting with regression, 9. Predicting with Regression Multiple Covariates"
author: "[Juan David Leong√≥mez](https://jdleongomez.info)"
date: "`r Sys.setlocale('LC_TIME','English');format(Sys.Date(),'%d %B, %Y')`"
output:
  bookdown::pdf_document2:
    toc: true
    fig_caption: yes
    highlight: zenburn
    number_sections: yes
    toc_depth: '5'
header-includes: \usepackage{fancyhdr}
  \pagestyle{fancy}
  \usepackage[dvipsnames]{xcolor}
  \definecolor{mygray}{gray}{0.8}
  \usepackage{float}
  \floatplacement{figure}{H}
editor_options: 
  chunk_output_type: console
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl
bibliography: Bibliography.bib
urlcolor: blue
link-citations: true
linkcolor: red
---

```{r setup, include=FALSE}
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(echo = TRUE)
opts_chunk$set(fig.width = 6, fig.height = 4, fig.pos = "H")
```

------------------------------------------------------------------------

# **Lecture 1**: `caret` package

The `caret` package [@caretpkg, currently v6.0-86] is a very useful front end package that wraps around a lot of the prediction algorithms and tools that you'll be using in the `R` programming language [manual: @kuhnCaretPackage2019; R documentation: @kuhnCaretPackageDocumentation2020].

## `caret` functionality

-   Some preprocessing (cleaning)

    -   [`preProcess`](https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/preProcess)

-   Data splitting (for cross-validation)

    -   [`createDataPartition`](https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/createDataPartition)
    -   `createResample`
    -   `createTimeSlices`

-   Training/testing functions

    -   [`train`](https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/train)
    -   `predict`

-   Model comparison (to see how well the models did in new datasets)

    -   [`confusionMatrix`](https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/confusionMatrix)

## Machine learning algorithms in `R`

There a large machine learning algorithms that are built into `R`, so these range from very popular statistical machine learning algorithms like:

-   Linear discriminant analysis

-   Regression

-   Naive Bayes

-   Support vector machines

-   Classification and regression trees

-   Random forests

-   Boosting

-   etc.

All of these algorithms are built by a variety of different developers, all coming from different backgrounds, so the interfaces that each of these sort of prediction algorithms is slightly different. Because of this, objects from different algorithms have different classes (see Fig. \@ref(fig:caretobjects)). In order to `predict`, is therefore usually necessary to add the `type` parameter.

```{r caretobjects, fig.align="center", fig.cap = "Different caret objects.", out.width = "100%"}
knitr::include_graphics("caret_objects.png")
```

## SPAM example: Data splitting

This a simple example. Here, after loading the packages and data, we partition the data into training and test sets. Here, we used 75% of our data to train the model and 25% to test, using the `createDataPartition` function.

### Sub-setting into training (75%) and testing (25%) datasets (`createDataPartition`)

Then, we create subsets for training and testing based on that partition

```{r}
library(caret)
library(kernlab)
data(spam)

inTrain <- createDataPartition(y = spam$type,
                               p = 0.75,
                               list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
dim(training)
```

### Fit model (`train`)

Now, we can fit a model using the `train` function. We used all variables (`~.`) as predictors of `type` (spam, nonspam). In this case, it used bootstrapping with 25 replicates, correcting for the potential bias that might come from bootstrap sampling.

```{r warning = FALSE}
RNGkind(sample.kind = "Rounding") #to make seed equivalent to older R versions
set.seed(32343)

modelFit <- train(type ~.,
                  data = training,
                  method = "glm")
modelFit
```

To look at the final model, you can look at the `$finalModel` component of the model fit.

```{r}
modelFit$finalModel
```

### Predict with the new model (`predict`)

Then, you can predict on a new samples by using the `predict` function (here, only the first 100 predictions).

```{r}
predictions <- predict(modelFit, 
                       newdata = testing)
predictions[1:100]
```

When you do that, it will give you a set of predictions that correspond to the responses, and you can use those to try to evaluate whether your model fit works very well or not. One way that you can do that is by calculating the confusion matrix (function `confusionMatrix)`, so that's using this confusion matrix function, and so note the capital M here.

### Confusion matrix (evaluate with `confusionMatrix`)

To evaluate the prediction from our model, we can compare the predictions on the testing set, against the actual outcome variable in that testing set.

```{r}
confmat <- confusionMatrix(predictions, testing$type)
confmat
```

This creates a table for which of the cases that you predicted to be nonspam are actually nonspam, which is the number of cases where it was spam, and you predicted to be spam and so forth.

```{r}
kable(confmat$table,
      booktabs = TRUE) %>%
  kable_styling(position = "center", latex_options = "HOLD_position")
```

And then it gives you a bunch of summary statistics like Sensitivity, Specificity, PPV, NPV, Accuracy:

```{r}
kable(confmat$byClass,
      booktabs = TRUE,
      col.names = "Value") %>%
  kable_styling(position = "center", latex_options = "HOLD_position")
```

#### Variable importance (***not in the lectures so far***)

Variable importance can be obtained using `varImp` on the model fitted using `train`. The `data.frame` it contains can be plotted in `ggplot`.

```{r fig.cap = "Importance of the variables from the spam model.", fig.height = 6, results = "Hold"}
library(ggpubr)

imp.df <- varImp(modelFit)$importance
ggplot(imp.df, aes(x = reorder(row.names.data.frame(imp.df), Overall), 
                   y = Overall,
                   fill = Overall)) +
  geom_bar(stat="identity", position="dodge") + 
  coord_flip() +
  ylab("Variable Importance") +
  xlab("") +
  ggtitle("Information Value Summary") +
  scale_fill_gradient(low="blue", high="red") +
  theme_pubclean()
```

## Recommended for further information

-   [@kuhnShortIntroductionCaret]

-   [@kuhnPredictiveModelingCaret2013]

-   [@kuhn2008]

------------------------------------------------------------------------

# **Lecture 2**: Data slicing

This lecture is about data slicing; you may use data slicing either for:

1.  Building your training and testing sets right at the beginning of your prediction function creation, or

2.  Performing cross validation or boot strapping within your training set, in order to evaluate your models

## SPAM example: Data splitting

Again, we use the `spam` dataset from the `kernlab` package, and split it to 75% for training, and 25% for testing.

```{r eval = FALSE}
library(caret)
library(kernlab)
data(spam)


inTrain <- createDataPartition(y = spam$type,
                               p = 0.75,
                               list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
dim(training)
```

## SPAM example: $k$-fold

We can also split the data for $k$-fold cross-validation, and just include the $k$ number, using the `createFolds` function.

```{r warning = FALSE}
RNGkind(sample.kind = "Rounding") #to make seed equivalent to older R versions
set.seed(32323)

folds <- createFolds(y = spam$type,
                     k = 10,
                     list = TRUE,
                     returnTrain = TRUE)

sapply(folds, length)
```

This splits the samples in order. For example, if we look at the first 10 elements of the fist sample:

```{r}
folds[[1]][1:10]
```

Alternatively, instead of returning the train sets (`returnTrain = TRUE`), you can ask the function to return the test sets (`returnTrain = FALSE`).

```{r warning = FALSE}
RNGkind(sample.kind = "Rounding") #to make seed equivalent to older R versions
set.seed(32323)

folds <- createFolds(y = spam$type,
                     k = 10,
                     list = TRUE,
                     returnTrain = FALSE)

sapply(folds, length)
```

Again, this splits the samples in order. For example, if we look at the first 10 elements of the fist sample:

```{r}
folds[[1]][1:10]
```

## SPAM example: Resampling

Resampling can be done using the `createResample` function, and specifying how many times you want to resample the data (e.g. `times = 10`).

```{r warning = FALSE}
RNGkind(sample.kind = "Rounding") #to make seed equivalent to older R versions
set.seed(32323)

folds <- createResample(y = spam$type,
                        times = 10,
                        list = TRUE)

sapply(folds, length)
```

And look at the elements selected. In this case, because we are resampling with replacement, the same element can be repeated:

```{r}
folds[[1]][1:10]
```

## SPAM example: Time Slices

Time slices can be done using the `createTimeSlices` function, and specifying that, for example, I want to create slices that have a window of about 20 samples in them (`initialWindow = 20`), and I want to say I'm going to predict the next 10 samples (`horizon = 10`)out after I take the initial window of 20.

```{r warning = FALSE}
RNGkind(sample.kind = "Rounding") #to make seed equivalent to older R versions
set.seed(32323)

tme <- 1:1000
folds <- createTimeSlices (y = tme,
                           initialWindow = 20,
                           horizon = 10)

names(folds)
```

If we look at the elements selected for training (`initialWindow`), we can see the first 20 elements have been selected for training (`folds$train`):

```{r}
folds$train[[1]]
```

And that the next 10 elements were selected for testing (`folds$test`):

```{r}
folds$test[[1]]
```

More information on Time Slicing is found on the recommended lectures [@kuhnShortIntroductionCaret; @kuhnPredictiveModelingCaret2013; @kuhn2008].

------------------------------------------------------------------------

# **Lecture 3**: Training options

This is a brief lecture about some of the training control options that you have when training while using the caret package. For this lecture, we're going to be using the spam example again just to illustrate how these ideas work.

```{r warning = FALSE}
library(caret)
library(kernlab)
data(spam)

inTrain <- createDataPartition(y = spam$type,
                               p = 0.75,
                               list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
```

Usually, what you would do when you fit a model is basically just use the train function, where you basically set all of the defaults to be whatever defaults that the train function chooses for you, specifying only the method `(method = "glm"`) that you're going to be using to fit and which data set you're going to be using (`data = training`).

```{r warning = FALSE}
modelFit <- train(type ~., 
                  data = training,
                  method = "glm")
```

## Train options

You can also use a large set of option for training.

```{r eval = FALSE}
args(train)

function(x, y, 
         method = "rf",
         preProcess = NULL, ...,
         weights = NULL,
         metric = ifelse(is.factor(y), "Accruracy", "RMSE"),
         maximise = ifesle(metric = "RMSE", FALSE, TRUE),
         trControl = trainConrol(),
         tuneGrid = NULL,
         tuneLength = 3)
```

### Metric options (`metric`)

**Continuous outcomes:**

-   `RSME` = Root Mean Squared Error

-   `RSquared` = $R^2$ from regression models

**Categorical outcomes**

-   `Accuracy` = Fraction correct

-   `Kappa` = A measure of concordance

### Train control (`trainControl`)

The `trainControl` argument allows you to be much more precise about the way that you train models.

```{r  eval = FALSE}
args(trainControl)

function (method = "boot", 
          number = ifelse(grepl("cv", method), 10, 25), 
          repeats = ifelse(grepl("[d_]cv$", method), 1, NA), 
          p = 0.75, 
          search = "grid", 
          initialWindow = NULL,
          horizon = 1, 
          fixedWindow = TRUE, 
          skip = 0, 
          verboseIter = FALSE, 
          returnData = TRUE, 
          returnResamp = "final", 
          savePredictions = FALSE,
          classProbs = FALSE, 
          summaryFunction = defaultSummary, 
          selectionFunction = "best",
          preProcOptions = list(thresh = 0.95, 
                                ICAcomp = 3, 
                                k = 5,
                                freqCut = 95/5, 
                                uniqueCut = 10, 
                                cutoff = 0.9), 
          sampling = NULL
          index = NULL, 
          indexOut = NULL, 
          indexFinal = NULL, 
          timingSamps = 0,
          predictionBounds = rep(FALSE, 2), 
          seeds = NA, 
          adaptive = list(min = 5,
                          alpha = 0.05,
                          method = "gls", 
                          complete = TRUE),
          trim = FALSE, allowParallel = TRUE)
```

### Train control (`trainControl`) resampling

-   `method`

    -   `"boot"` = bootstrapping

    -   `"boot632"` = bootstrapping with adjustemnt

    -   `"cv"` = cross-validation

    -   `"repeatedcv"` = repeated cross-validation

    -   `"LOOCV"` = leave one out cross-validation

-   `number`

    -   For boot/cross-validation

    -   Number of subsamples to take

-   `repeats`

    -   Number of times to repeat subsampling

    -   If big this can considerably slow things down

In general the defaults work pretty well, but if you have large numbers of samples or you have a model that requires fine tuning across a large number of parameters, you may want to increase for example the number of cross-validation or bootstrap samples that you take.

### Setting the seed

-   It is often useful to set an overall seed

-   For parallel fits, you can also set a seed for each sample (and it is most useful in this case)

```{r warning = FALSE}
RNGkind(sample.kind = "Rounding") #to make seed equivalent to older R versions
set.seed(1235)

modelFit2 <- train(type ~.,
                   data = training,
                   method = "glm")
modelFit2
```

## Further resources

-   [Caret tutorial](http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf) [-@kuhn2008]

-   [Model training and tuning](https://topepo.github.io/caret/model-training-and-tuning.html) [-@kuhnCaretPackage2019]

------------------------------------------------------------------------

# **Lecture 4**: Plotting predictions

One of the most important components of building a machine learning algorithm or prediction model is understanding how the data actually look and how the data interact with each other.

The best way to do that is actually plotting the data, and in particular plotting the predictors.

We will use the `Wage` dataset, from the `ISLR` package, from the book *Introduction to Statistical Learning* [@jamesIntroductionStatisticalLearning2013].

```{r}
library(ISLR)
library(ggplot2)
library(ggpubr)
library(caret)

data(Wage)
summary(Wage)
```

Just by looking at the `summary` of the data, some interesting characteristics are obvious; for example, that the sample is only of males.

**Get training/test sets**

```{r}
inTrain <- createDataPartition(y = Wage$wage,
                               p = 0.7,
                               list = FALSE)

training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training); dim(testing)
```

## Feature plot (`caret` package)

```{r fig.cap = "The \\texttt{featurePlot} from the \\texttt{caret} package. In this case, we asked for the association of three predictors (\\texttt{age, education, jobclass}) with the dependent variable (\\texttt{wage}), as well as amongst them. In particular, you need to look at the first row (i.e. the assiociations between the predictors and the outcome variable); in this case, for example, there is a clear association between \\texttt{ajobclass} and \\texttt{wage}.", results = "Hold"}
featurePlot(x = training[, c("age", "education", "jobclass")],
            y = training$wage,
            plot = "pairs")
```

## Qplot (*ggplot2* package)

```{r fig.cap = "The \\texttt{qplot} from the \\texttt{ggplot2} package. In this case, we asked for the association between \\texttt{age} and \\texttt{wage}.", results = "Hold"}
qplot(age, wage, data = training) + 
  theme_pubclean()
```

While there seems to be a clear association, it is also apparent that there are two `chunks`; the bottom one (with most of the data) shows some association, but the top one is clearly different. To try to understand this weird situation, we can plot by another variable (in this case, `jobclass`).

```{r fig.cap = "The \\texttt{qplot} from the \\texttt{ggplot2} package. In this case, we asked for the association between \\texttt{age} and \\texttt{wage} by \\texttt{jobclass}.", results = "Hold"}
qplot(age, wage, colour = jobclass, data = training) + 
  theme_pubclean()
```

This shows that most of the points in the *weird* chunk, are for people on *information* jobs. This gives you a way to detect variables that might be important in your model, as they show, variation in the data.

## Add regression smoothers (*ggplot2* package)

```{r fig.cap = "The \\texttt{qplot} from the \\texttt{ggplot2} package. In this case, we asked for the association between \\texttt{age} and \\texttt{wage} by \\texttt{education}, including regression lines for each \\texttt{education} level.", results = "Hold"}
qplot(age, wage, colour = education, data = training) +
  geom_smooth(method = "lm") + 
  theme_pubclean()
```

## `cut2`, making factors (`Hmisc` package)

the `cut2` function from the `Hmisc` package, allows us to break up things like the wage variable into different categories cause sometimes it's clear that specific categories seem to have different relationships.

```{r}
library(Hmisc)
cutWage <- cut2(training$wage, g = 3)
table(cutWage)
```

Dividing a predictor into factor levels, can help understand trends. For example:

```{r fig.cap = "Boxplots \\texttt{qplot} for the association between \\texttt{age} and our new 3-level factor \\texttt{cutWage}.", results = "Hold"}
qplot(cutWage, age, fill = cutWage, data = training, geom = "boxplot") + 
  theme_pubclean()
```

Adding the point can also help how many there are in each category. In the class they use `jitter` without adding an `alpha` value to these, so they make two panels. Here, I used `alpha` to the jittered points, so panel B should be enough.

```{r fig.cap = "Boxplots \\texttt{qplot} for the association between \\texttt{age} and our new 3-level factor \\texttt{cutWage}. \\textbf{A.} Boxplots without points. \\textbf{B.} Boxplots with jittered and somewhat transparent points.", results = "Hold"}
library(ggpubr)

p1 <- qplot(cutWage, age, fill = cutWage, data = training, geom = "boxplot") + 
  theme_pubclean()
p2 <- p1 +
  geom_jitter(alpha = .2) + 
  theme_pubclean()

ggarrange(p1, p2,
          ncol = 2,
          common.legend = TRUE,
          legend = "bottom",
          labels = "AUTO")
```

Because there are lots of points in each `cutWage` category, any trend we see is likely real.

## Tables

Another useful option is to look at tables. For example:

```{r}
t1 <- table(cutWage, training$jobclass)
t1
```

It is clear that, for example, there are more industrial jobs in the lower wage variable than there are information jobs. And that trend reverses itself for the highway jobs. This can also be converted to proportions using the `prop.table` function (the number **1**, refers to proportions per row; a number 2 would calculate proportions per column):

```{r}
prop.table(t1, 1)
```

## Density plots

Another way to look at the data is with densities. Interestingly, in this case there is a second peak in high wages for people with more education.

```{r fig.cap = "Density \\texttt{qplot} according to education level.", results = "Hold"}
qplot(wage, color = education, data = training, geom = "density") + 
  theme_pubclean()
```

But (and this is MY idea), it is mainly for people in information jobs: interestingly, there is a second peak for people with more education.

```{r fig.cap = "Density \\texttt{qplot} according to education level, split by \\texttt{jobclass}.", results = "Hold"}
qplot(wage, color = education, data = training, geom = "density") +
  facet_wrap(~jobclass) + 
  theme_pubclean()
```

## Notes and further reading

-   Make your [**plots only in the training set**]{.ul}

    -   Don't use the test set for exploration!

-   [**Things you should be looking for**]{.ul}

    1.  Imbalance in outcomes/predictors

    2.  Outliers

    3.  Groups of points not explained by a predictor

    4.  Skewed variables

**Further Reading**

-   [ggplot2 tutorial](http://rstudio-pubs-static.s3.amazonaws.com/2176_75884214fc524dc0bc2a140573da38bb.html)

-   [caret visualizations](http://caret.r-forge.r-project.org/visualizations.html)

------------------------------------------------------------------------

# **Lecture 5**: Basic preprocessing

This lecture's about preprocessing predictor variables. It is important to plot the variables upfront so you can see if there's any sort of weird behaviour of those variables. Sometimes predictors will look very strange or the distribution will be very strange, and you might need to transform them in order to make them more useful for prediction algorithms.

This is particularly true when you're using model based algorithms, such as linear discriminate analysis, naive Bayes, or linear regression. Pre-processing can be more useful often when you're using model based approaches, then when you're using more non parametric approaches.

## Why pre-process?

When deciding how to preprocess data, or how to explore data, we **only look at the training set**.

```{r results = "Hold", fig.cap = "Histogram of the \\texttt{capitalAve} variable. The distribution is far from normal."}
library(caret)
library(kernlab)
library(ggplot2)
library(ggpubr)

data(spam)

inTrain <- createDataPartition(y = spam$type,
                               p = 0.75,
                               list = FALSE)

training <- spam[inTrain,]
testing <- spam[-inTrain,]

gghistogram(training, x = "capitalAve",
            fill = "blue",
            color = "blue",
            add = "mean", 
            rug = TRUE,
            bins = 12,
            palette = "blue",
            add_density = TRUE) +
  labs(x = "Average capital run length",
       y = "Frequency") + 
  theme_pubclean()
```

For the `capitalAve` variable, for example, the mean is `r round(mean(training$capitalAve), 2)`, but the standard deviation is much larger: `r round(sd(training$capitalAve), 2)`.

In this case, it is important to pro-process this variable, so that the machine learning algorithm does not get tricked by the fact the variable is skewed and highly variable.

## Standardising

One option is to transform into $Z$ scores. Centring and scaling is one approach, and that will take care of some the problems that we see in these data.E.g.:

```{r}
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve - mean(trainCapAve))/sd(trainCapAve)
```

Now, obviously the mean is `r round(mean(trainCapAveS), 2)`, and the standard deviation is `r round(sd(trainCapAveS), 2)`.

### Standardising - test set

Now, when we applied the same standardisation to the testing data set, we do it **based on the mean and SD of the training set**.

```{r results = "Hold"}
testCapAve <- testing$capitalAve
testCapAveS <- (testCapAve - mean(trainCapAve))/sd(trainCapAve)
```

Because of this, mean will not equal 0, and SD will not equal 1, but hopefully these values will not be far off. In fact, when doing this for the testing set, the mean is `r mean(testCapAveS)`, and the standard deviation is `r round(sd(testCapAveS), 2)`.

### Standardising - `preProcess` function

Alternatively, `caret` has a `preProcess` function. Here, we pass all the predictor variables from the data set (except fro the number 58, which is the outcome variable we are trying to predict).

```{r}
preObj <- preProcess(training[,-58],
                     method = c("center", "scale"))
trainCapAveS <- predict(preObj,
                        training[,-58])$capitalAve
```

Here, again, the mean is `r round(mean(trainCapAveS), 2)`, and the standard deviation is `r round(sd(trainCapAveS), 2)`.

We can also apply this to the test set:

```{r}
testCapAveS <- predict(preObj,
                      testing[,-58])$capitalAve
```

Here, the mean is `r mean(testCapAveS)`, and the standard deviation is `r round(sd(testCapAveS), 2)`.

### Standardising - `preProcess` as argument (in the `train` function)

`preProcess` can also be passed as an argument to the `train` function.

```{r message = FALSE, warning = FALSE}
RNGkind(sample.kind = "Rounding") #to make seed equivalent to older R versions
set.seed(32343)

modelFit <- train(type ~.,
                  data = training,
                  preProcess = c("center", "scale"),
                  method = "glm")
modelFit
```

## Standardising - Box-Cox transforms

Alternatively, you can use other transformations, like Box-Cox transforms. These are a set of transformations that take continuous data, and try to make them look like normal data, by estimating a specific set of parameters using maximum likelihood.

```{r warning = FALSE, fig.cap = "Distribution of the Box-Cox transformed \\texttt{trainCapAveS} variable. \\textbf{A.} Histogram. \\textbf{B.} Q-Q plot."}
preObj <- preProcess(training[,-58],
                     method = c("BoxCox"))
trainCapAveS <- predict(preObj,
                        training[,-58])$capitalAve

p1 <- gghistogram(cbind(training,trainCapAveS), x = "trainCapAveS",
            fill = "red",
            color = "red",
            add = "mean", 
            rug = TRUE,
            bins = 17,
            palette = "blue") +
  labs(x = "Average capital run length",
       y = "Frequency") + 
  theme_pubclean()
p2 <- ggqqplot(cbind(training, trainCapAveS), x = "trainCapAveS",
               color = "red",
               ggtheme = theme_pubclean())

ggarrange(p1, p2,
          labels = "AUTO",
          ncol = 2)
```

In the Q-Q plot is clear how values that are repeated cannot be *fixed* with that transformation (in this case, a bunch of zeroes).

## Standardising - Imputing data

It is obviously common to have missing data. In such cases, prediction algorithms often fail. We can use `method = "knnImpute"` in the `preProcess` function. `"knnImpute"` refers to **k-nearest neighbours (KNN)** imputation, a simple machine learning algorithm that computes the k. So if k equal to ten, then it takes the 10 nearest, data vectors that look most like data vector with the missing value, and average the values of the variable that is missing and compute them at that position.

Here, we do that, and then standardise the values (to $Z$ scores).

```{r message = FALSE, warning = FALSE}
RNGkind(sample.kind = "Rounding") #to make seed equivalent to older R versions
set.seed(13343)

# Make some NA values
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1], size = 1, prob = 0.05) == 1
training$capAve[selectNA] <- NA

#Impute and standardise
preObj <- preProcess(training[,-58],
                     method = "knnImpute")
capAve <- predict(preObj,
                  training[,-58])$capAve

#Standardise true values
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth - mean(capAveTruth))/sd(capAveTruth)
```

We can now compare the quantiles of the true values, versus the imputed ones

```{r}
quantile(capAve - capAveTruth)
```

We can also check only the values that were imputed (`NAs`), and compare them to the values that were not `NA`. Here, we can see that values that were imputed are a little bit farther appart from 0, compare to the ones that were not, but not too much.

```{r}
quantile((capAve - capAveTruth)[selectNA])
quantile((capAve - capAveTruth)[!selectNA])
```

## Notes and further reading

-   Training and test sets must be **processed in the same way:** The `caret` package handles a lot of this under the hood in the sense that if you train a data set using `preProcess` functions, built into the `train` function, it applies that preprocessed function to the test set. It will handle all of the preprocessing in the correct way for you

-   **Test transformation will likely be imperfect**

    -   Especially if the test/training sets were collected at different times
    -   But, you need to make sure any transformation you apply to the training set, is done in the same way to the test set

-   Be [**careful when transforming factor variables!**]{.ul} It is much more difficult to know how what is the right transformation. Most machine learning algorithms are built to deal with either:

    -   binary predictors (in which case the binary predictors are not pre-processed), or

    -   continuous predictors in which case sometimes it's expected that, the data are preprocessed to look more normal

-   [preprocessing with caret](https://topepo.github.io/caret/pre-processing.html)

------------------------------------------------------------------------

# **Lecture 6**: Covariate creation

This lecture is about covariate creation. Covariates are sometimes called predictors and sometimes called features. They're the variables that you will actually include in your model that you're going to be using to combine them to predict whatever outcome that you care about.

There are two levels of covariate creation, or feature creation:

1.  The first level is, taking the raw data that you have and turning it into a predictor that you can use. So the raw data often takes the form of an image, or a text file, or a website. That kind of information is very hard to build a predictive model around when you have not summarised the information in some useful way into either a quantitative or qualitative variable

    -   what we want to do is take that raw data and turn it into features or covariates which are variables that describe the data as much as possible while giving some compression and making it easier to fit standard machine-learning algorithms

    -   in this step, the raw data of the covariate, usually involves a lot of thinking about the structure of the data that you have and what is the right way to extract, extract the most useful information in the fewest number of variables that captures everything that you want

2.  Transforming tidy covariates

```{r featlevels, fig.align = "center", fig.cap="The two levels of covariate (or feature) creation, with the example of an email.", out.width = "100%"}
knitr::include_graphics("featlevels.png")
```

**First level:** In this example of an email, first, the average number of capitals (actually, the [proportion]{.ul}) that are in the email (in this case 100% of the letters in the email are capital letters); second, frequency a particular word appears (for example, how often does *you* appear?); third, number of dollar signs (this might be a really good predictor of whether an email is spam or not). Here you can see there are a large number of dollar signs (8), so we calculated another feature of that data set.

**Second level:** it might not be the average number what is related very well to the outcome that we care about; it might be the average number of capitals squared or cubed, or it might be some other function of that. So, the next stage is transforming the variables into more useful variables.

## Level 1: Raw data \<- covariates

-   Depends heavily on application

-   The balancing act is summarisation vs. information loss

    -   the best features are features that capture only the relevant information in, say, the image or the email, and throw out all the information that's not really useful at all. And so the idea is that you have think very carefully about how to pick the right features that explain most of what's happening in your raw data.

-   Examples:

    -   Text files: frequency of words, frequency of phrases (a good example is [Google ngrams](https://books.google.com/ngrams))[,](https://books.google.com/ngrams)),) frequency of capital letters

    -   Images: Edges, corners, blobs, ridges ([computer vision feature detection)](http://en.wikipedia.org/wiki/Feature_detection_(computer_vision))

    -   Webpages: Number and type of images, position of elements, colours, videos ([A/B Testing](http://en.wikipedia.org/wiki/A/B_testing))

    -   People: Height, weight, hair colour, sex, country of origin

-   The more knowledge of the system you have the better the job you will do

-   When in doubt, err on the side of creating more features

    -   it is **better to create more features, and then filter them out** in the model-building process

-   Can be automated, but use caution!

    -   use a lot of caution when using that approach because sometimes a particular feature will be very useful in the training set that you created but won't be very useful in a new set of data and the test set; it won't generalize well

## Level 2: Tidy covariates \<- new covariates

The second level is taking tidy covariates (features you've already created on the data set, and then creating new covariates out of them. Usually this is transformations or functions of the covariates, that might be useful when building a prediction model.

-   More necessary for some methods (regression, svms) than for others (classification trees).
-   Should be done ***only on the training set***
-   The best approach is through exploratory analysis (plotting/tables)
-   When using the `caret` package, these **new covariates should be added to data frames**

## Example

### Load data

```{r loadData,cache=TRUE}
library(ISLR)
library(caret)

data(Wage)

inTrain <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
```

### Common covariates to add, dummy variables

**Basic idea:** convert factor variables to [indicator variables](http://bit.ly/19ZhWB6).

In this case let's look in the training set at the variable called job class. So that job class has two different levels, it's either industrial, or it's information. So one thing that we could try to do is try to plug that variable directly into a prediction model, but the values of that variable will be a actually a set of characters. It'll either be "industrial", or it'll be "information".

```{r dummyVar1,dependson="loadData"}
table(training$jobclass)
```

And it's sometimes hard for prediction algorithms to use those qualitative information variables, in order to actually do the prediction. So one thing we might want to do is turn it into a quantitative variable, and the way that you can do that with the `caret` package is with this `dummyVars` function. So basically it says we're going to pass in a model so the outcome is wage. Job class is going to be the predictor variable, and the training set is the set where we're going to be building those dummy variables. And then if you predict, using the `predict` function, this dummy's object and a new data set, in this case we're just going to apply it to the training data set, you get, two new variables out.

```{r dummyVar,dependson="loadData"}
dummies <- dummyVars(wage ~ jobclass,data=training)
head(predict(dummies,newdata=training))
```

So the first is an indicator that you are *industrial*, and the second is an indicator that you're *information*. If the indicator of industrial is 1, it means that for that person has an industrial job. If it's 0, it means for that person, they had not an industrial job. So the same thing is true for information.

So, in this case, where's there only two different levels of this variable, there's only industrial and information, then whenever you're 1 for industrial, you're 0 for information, and whenever you're 0 for industrial, you're 1 for information and so forth.

But if you had three variables here, it would probably have, every column would have two zeros, because those are the two classes you don't belong to, and a one for the class that you belong to. So this is taking these factor or qualitative variables and turning them into quantitative variables.

### Removing zero covariates

Another thing that happens is that some of the variables are basically have no variability in them.

So it's often that you'll create a feature for example, if you create a feature that says for emails, does it have any letters in it at all? Almost every single email will have lots, have at least one letter in it, so that variable will always be equal to true. It's always got letters in it, so it has no variability and it's probably not going to be a useful covariate.

```{r ,dependson="dummyVar"}
nsv <- nearZeroVar(training,saveMetrics=TRUE)
nsv
```

For example, here we can see that it tells us the percentage of unique values for a particular variable, so in this case the variable has about 0.33% unique values, and it's not near zero variance variable, but for example, the variable sex, only is basically males and so it has a very low frequency ratio.

In other words, it's basically all one category, and so, this ends up being a near zero variable and so, you could use this column of the matrix to throw out all those variables like sex and, in this case, region, that are variables that don't really have any variability in them and shouldn't be used in prediction algorithms. So this is a nice way to throw those less meaningful predictors out right away.

## Spline basis

If you do linear regression or generalized linear regression as your prediction algorithm (future lecture), the idea will be to fit, basically straight lines through the data.

Sometimes, you want to be able to fit curvy lines, and one way to do that is with a `basis` functions, and so you can find those, for example, in the `splines` package.

The `bs` function will create a polynomial variable. So in this case, we pass at a single variable, in this case, the training set, we take the $Age$ variable, and we say we want a third degree polynomial for this variable (`df = 3`).

```{r}
library(splines)

bsBasis <- bs(training$age,
              df = 3)
head(bsBasis)
```

So when you do that, you essentially get, you'll get a three-column matrix out. So this is now three new variables:

1.  The first variable corresponds to age, the actual $Age$ values (but scaled for computational purposes)

2.  $Age^2$

3.  $Age^3$

If you include these covariates in the model instead of just the age variable when you're fitting a linear regression, you allow for curvy model fitting.

## Fitting curves with splines

We can fit a model polynomial model (`lm1`) with the variables created from $Age$.

```{r}
lm1 <- lm(wage ~ bsBasis, 
          data = training)
summary(lm1)
```

And plot this model:

```{r fig.cap = "Model to predict Wage from Age. In this case, we used a model with the polynomial model: $Age$, $Age^2$, and $Age^3$ as predictors. The red line represents the model.", fig.height = 6, results = "Hold"}

#In base R
#plot(training$age, training$wage,
#     pch = 19,
#     cex = 0.5)
#points(training$age,
#       predict(lm1, newdata = training),
#       col = "red",
#       pch = 19,
#       cex = 0.5)

# In ggplot2
ggplot(training, aes(x = training$age, y = training$wage)) +
  geom_point(alpha = 0.3) +
  geom_line(aes(y = predict(lm1, newdata = training)),
            color = "red")  + 
  labs(x = "Age", y = "Wage") +
  theme_pubclean()
```

### Splines on the test set

Then on the test set, you'll have to predict those same variables.

In this example, we predict $Age$ in the testing set, using the exact same procedure used on the training set. So you can do that by saying I'm going to predict from this variable that I created using the BS function:

```{r}
bsBasisAgeTesting <- predict(bsBasis,
                             age = testing$age)
head(bsBasisAgeTesting)
```

## Notes and further reading

-   Level 1 feature creation (raw data to covariates)

    -   Science is key. Google "feature extraction for [data type]"

    -   Err on overcreation of features

    -   In some applications (images, voices) automated feature creation is possible/necessary

        -   <http://www.cs.nyu.edu/~yann/talks/lecun-ranzato-icml2013.pdf>

-   Level 2 feature creation (covariates to new covariates)

    -   The function *preProcess* in *caret* will handle some preprocessing.
    -   Create new covariates if you think they will improve fit
    -   Use exploratory analysis on the training set for creating them
    -   Be careful about overfitting!

-   [preprocessing with caret](https://topepo.github.io/caret/pre-processing.html)

-   If you want to fit spline models, use the *`gam`* method in the *caret* package which allows smoothing of multiple variables.

-   More on feature creation/data tidying in the Obtaining Data course from the Data Science course track.

------------------------------------------------------------------------

# **Lecture 7**: Preprocessing with PCA

This is a lecture a lecture about preprocessing covariants with principal components analysis.

You have multiple quantitative variables that sometimes are highly correlated with each other. In other words, they are very similar to being the almost the exact same variable. In this case, it's not necessarily useful to include every variable in the model. You might want to include some summary that captures most of the information in those quantitative variables.

## Correlated predictors

Here, we are looking at the absolute values (`abs`) of the correlations between all variables (excluding the dependant variable), and selecting those that are \> 0.8.

```{r}
library(caret)
library(kernlab)

data(spam)

inTrain <- createDataPartition(y = spam$type,
                               p = 0.75,
                               list = FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]

M <- abs(cor(training[,-58]))
diag(M) <- 0
which(M > 0.8,
      arr.ind=T)
```

In the slides, they only get 1 large correlation (between `num415`, and `num857`; 2 rows). I, however, get 3:

-   `num415` and `num857`

-   `direct` and `num857`

-   `direct` and `num415`

```{r fig.cap = "Correlations with $r > 0.8$ between predictor variables in the \\texttt{spam} database. In the lectures, only panel \\textbf{A} appears, but I found these 3 correlations between the same 3 variables.", fig.height = 3, results = "Hold"}
p1 <- ggplot(spam, aes(x = num415, y = num857)) +
  geom_point(alpha = 0.3, color = "red") +
  theme_pubclean()
p2 <- ggplot(spam, aes(x = direct, y = num857)) +
  geom_point(alpha = 0.3, color = "purple") +
  theme_pubclean()
p3 <- ggplot(spam, aes(x = direct, y = num415)) +
  geom_point(alpha = 0.3, color = "blue") +
  theme_pubclean()

ggarrange(p1, p2, p3,
          ncol = 3,
          labels = "AUTO")
```

## Basic PCA idea

-   We might not need every predictor

-   A weighted combination of predictors might be better

-   We should pick this combination to capture the "most information" possible

-   Benefits

    -   Reduced number of predictors
    -   Reduced noise (due to averaging)

## We could rotate the plot {#whyrotate}

So just as an example, here's a combination I could do. I could say I could take 0.71 (check section \@ref(prcomp)) times the 415 variable plus 0.71 times 857 variable. And create a new variable called x. Which is basically the sum of those two variables. Then I could take the difference of those two variables. By basically doing 0.71 times 415 minus 0.71 times 857. So this is basically adding, x is adding the two variables together, y is subtracting the two variables.

$$X = 0.71 \times num415 + 0.71 \times num857$$

$$Y = 0.71 \times num415 - 0.71 \times num857$$

```{r fig.cap = "Rotated correlations.", fig.height = 3, fig.width = 3, results = "Hold"}
X <- 0.71*training$num415 + 0.71*training$num857
Y <- 0.71*training$num415 - 0.71*training$num857 

ggplot(data.frame(cbind(X, Y)), aes(x = X, y = Y))  +
  geom_point(alpha = 0.3, color = "purple") +
  theme_pubclean()
```

So then if I plot those variables versus each other, when I add them up, that's the x-axis, and when I take the difference, that's the y-axis. And so you can see most of the variability is happening in the x-axis.

In other words there's lots of points all spread out across the x-axis, but most of the points are clustered right here at 0 on the y-axis. So that almost all of these points have a y value of 0.

So, the adding the two variables together captures most of the information in those two variables and subtracting the variables takes less information. So the idea here is we might want to use the sum of the two variables as a predictor.

## Related problems

You have multivariate variables $X_1,\ldots,X_n$ so $X_1 = (X_{11},\ldots,X_{1m})$

-   Find a new set of multivariate variables that are uncorrelated and explain as much variance as possible.

    -   In other words from the previous plot, we're looking for the x variable which has lots of variation in it. And not the y variable which is almost always 0.

-   If you put all the variables together in one matrix, find the best matrix created with fewer variables (lower rank) that explains the original data.

The first goal is \textcolor{purple}{\textbf statistical} and the second goal is \textcolor{orange}{\textbf data compression}.

## Related solutions - PCA/SVD

**SVD** (Singular value decomposition)

If $X$ is a matrix with each variable in a column and each observation in a row then the SVD is a "matrix decomposition"

$$ X = UDV^T$$

where the columns of $U$ are orthogonal (left singular vectors), the columns of $V$ are orthogonal (right singular vectors) and $D$ is a diagonal matrix (singular values).

**PCA**

The principal components are equal to the right singular values ( $V$ ) if you first scale (subtract the mean, divide by the standard deviation) the variables.

## PCA in R - `prcomp` {#prcomp}

PCA between `num415`and `num857`.

```{r fig.cap = "PCA between \\texttt{num415} and \\texttt{num857}.", fig.height = 3, fig.width = 3, results = "Hold"}
smallSpam <- spam[,c(34,32)]
prComp <- prcomp(smallSpam)

ggplot(data.frame(prComp$x), aes(x = PC1, y = PC2))  +
  geom_point(alpha = 0.3, color = "red") +
  theme_pubclean()
```

You can check the rotation as well (hence the 0.71 used in section \@ref(whyrotate).

```{r}
round(prComp$rotation, 4)
```

PC1 is 0.7081 $\times$ `num415`, and 0.7061 $\times$ `num857`. The component that explains the most variance (PC1) is adding the 2 variables, and PC2 is the subtraction of these.

## PCA on SPAM data

I've applied a function of the data set, the $log_{10}$ transform, and added 1. I've done this to make the data look a little bit more Gaussian (some of the variables are skewed).

```{r fig.cap = "PCA on the SPAM data using \\texttt{prcomp}.", fig.height = 6, fig.width = 6, results = "Hold"}
prComp <- prcomp(log10(spam[,-58] + 1))

ggplot(data.frame(prComp$x), aes(x = PC1, y = PC2, color = spam$type))  +
  geom_point(alpha = 0.3) +
  labs(color = "Spam type") +
  theme_pubclean()
```

## PCA with `caret`

PCA can be achieved with `caret`, by using the `method = "pca"` argument in the `preProcess` function. To obtain the PCA scores, the `predict` function must be used.

```{r fig.cap = "PCA on the SPAM data using \\texttt{caret}.", fig.height = 6, fig.width = 6, results = "Hold"}
preProc <- preProcess(log10(spam[,-58]+1),
                      method = "pca",
                      pcaComp = 2)

spamPC <- predict(preProc,log10(spam[,-58]+1))

ggplot(spamPC, aes(x = PC1, y = PC2, color = spam$type))  +
  geom_point(alpha = 0.3) +
  labs(color = "Spam type") +
  theme_pubclean()
```

### Preprocessing with PCA

Then, you can use the PCA scores to `train` a model:

```{r warning = FALSE, message = FALSE}
preProc <- preProcess(log10(training[,-58]+1),
                      method = "pca",
                      pcaComp = 2)

trainPC <- predict(preProc,log10(training[,-58]+1))

# The normal syntax doesn't work
#modelFit <- train(training$type ~.,
#                  method="glm",
#                  data=trainPC)

# This syntax works
modelFit <- train(x = trainPC, y = training$type, method="glm")
```

Obviously, in the test data set, you must use the same principal components calculated in the training data set. To do this, we first `predict` PCA scores on the testing data set, using the `preProc` object created earlier. Then, we can create the `confusionMatrix` for the testing set, using the `modelFit` model, and the PCA scores on the `testPC` object.

```{r}
testPC <- predict(preProc,
                  log10(testing[,-58] + 1))

confusionMatrix(testing$type,
                predict(modelFit, testPC))
```

Here, we calculated a relatively small number of principal components, but still have a relatively high accuracy in prediction. So principal component analysis can reduce the number of variables while maintaining accuracy.

## Alternative (sets \# of PCs)

We can also add the PCA straight into the `train` function, in the `preProcess` argument.

```{r warning = FALSE, message = FALSE}
# The normal syntax doesn't work
#modelFit <- train(training$type ~.,
#                  method = "glm",
#                  preProcess = "pca",
#                  data = training)

# This syntax works
modelFit <- train(x = training[,-58], y = training$type, 
                  method="glm",
                  preProcess = "pca")

confusionMatrix(testing$type,
                predict(modelFit,testing))
```

## Final thoughts on PCs

-   Most useful for linear-type models

-   Can make it harder to interpret predictors

-   Watch out for outliers!

    -   Transform first (with logs/Box Cox)
    -   Plot predictors to identify problems

-   For more info see

    -   Exploratory Data Analysis
    -   [Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) [@hastieElementsStatisticalLearning2009]

------------------------------------------------------------------------

# **Lecture 8**: Predicting with regression

This lecture's about one of the most direct and simple ways to perform machine learning using **regression modelling**.

## Key ideas

-   Fit a simple regression model
-   Plug in new covariates and multiply by the coefficients
-   Useful when the linear model is (nearly) correct

**Pros**:

-   Easy to implement

-   Easy to interpret

**Cons**:

-   Often poor performance in nonlinear settings

## **Example: Old faithful eruptions**

This example is based on the Old Faithful Geyser Data (aka `faithful` database), that only contains 2 variables.

```{r warning = FALSE}
library(caret)

data(faithful)

RNGkind(sample.kind = "Rounding") #to make seed equivalent to older R versions
set.seed(333)

inTrain <- createDataPartition(y = faithful$waiting,
                               p = 0.5,
                               list = FALSE)

trainFaith <- faithful[inTrain,]
testFaith <- faithful[-inTrain,]

head(trainFaith)
```

As a plot:

```{r fig.cap = "Association between \\texttt{eruptions} and \\texttt{wating}.", fig.height = 4, fig.width = 4, results = "Hold"}
ggplot(trainFaith, aes(x = waiting, y = eruptions))  +
  geom_point(alpha = 0.3,
             color = "darkgreen") +
  labs(x = "Waiting Time (m)", y = " Eruption Duration (m)") +
  theme_pubclean()
```

### Fit a linear model

$$ED_{i} = b_{0} + b_{1}WT_{i} + e_{i}$$

```{r}
lm1 <- lm(eruptions ~ waiting,
          data = trainFaith)
summary(lm1)
```

The model is:

```{r fig.cap = "Linear regression between \\texttt{eruptions} and \\texttt{wating}.", fig.height = 4, fig.width = 4, results = "Hold"}
ggplot(trainFaith, aes(x = waiting, y = eruptions))  +
  geom_line(aes(y = predict(lm1)),
            color = "darkgreen") +
  geom_point(alpha = 0.3,
             color = "darkgreen") +
  labs(x = "Waiting Time (m)", y = " Eruption Duration (m)") +
  theme_pubclean()
```

To predict a value:

$\hat{ED} = \hat{b}_{0} + \hat{b}_{1}WT$[^1]

[^1]: The hat ($\hat{}$): In [simple linear regression](https://en.wikipedia.org/wiki/Simple_linear_regression "Simple linear regression") with observations of independent variable data $x_{i}$ and dependent variable data $y_{i}$, and assuming a model of $y_{i} = \beta _{0} + \beta _{1}x _{1} + \varepsilon _{i}$, can lead to an estimated model of the form $\hat {y}_{i}={\hat {\beta }}_{0}+{\hat {\beta }}_{1}x_{i}$ where $\sum _{i}(y_{i}-{\hat {y}}_{i})^{2}$ is minimized via [least squares](https://en.wikipedia.org/wiki/Least_squares "Least squares") by finding optimal values of $\hat {\beta }_{0}$ and $\hat {\beta }_{1}$ for the observed data.

Here, there is no error term as we do not know what the error is for a particular value (in this example, 80).

```{r}
coef(lm1)[1] + coef(lm1)[2] * 80
```

Or

```{r}
newdata <- data.frame(waiting=80)
predict(lm1,newdata)
```

### Plot predictions: training and test

```{r fig.cap = "Linear regression between \\texttt{eruptions} and \\texttt{wating} from the model applied to the trainig (\\textbf{A}) and testing (\\textbf{B}) datasets. On the testing (\\textbf{B}) dataset, the regression line is a little tilted, but that is expected as the model was not built with these specific data.", fig.height = 4, results = "Hold"}
p1 <- ggplot(trainFaith, aes(x = waiting, y = eruptions)) +
  geom_line(aes(y = predict(lm1)),
            color = "darkgreen") +
  geom_point(alpha = 0.3,
             color = "darkgreen") +
  labs(x = "Waiting Time (m)", y = " Eruption Duration (m)") +
  theme_pubclean()
p2 <- ggplot(testFaith, aes(x = waiting, y = eruptions)) +
  geom_line(aes(y = predict(lm1, newdata = testFaith)),
            color = "red") +
  geom_point(alpha = 0.3,
             color = "red") +
  labs(x = "Waiting Time (m)", y = " Eruption Duration (m)") +
  theme_pubclean()

ggarrange(p1, p2,
          labels = "AUTO")
```

## Get training/test set errors

**RMSE on the training dataset**

```{r}
sqrt(sum((lm1$fitted.values - trainFaith$eruptions)^2))
```

**RMSE on the test dataset**

Obviously, this is a more realistic estimation of the (larger) error that you would get on a new data set compared to the value that we got on the training set.

```{r}
sqrt(sum((predict(lm1, newdata = testFaith) - testFaith$eruptions)^2))
```

## Prediction intervals

**In base R**

```{r fig.cap = "Linear regression between \\texttt{eruptions} and \\texttt{wating} from the model applied to testing dataset, with prediction intervals, using base R.", fig.height = 4, results = "Hold"}
pred1 <- predict(lm1,
                 newdata = testFaith,
                 interval="prediction")

ord <- order(testFaith$waiting)

plot(testFaith$waiting,testFaith$eruptions,
     pch = 19,
     col = "blue")
matlines(testFaith$waiting[ord], pred1[ord,],
         type = "l",
         col = c(1,2,2),
         lty = c(1,1,1), 
         lwd = 3)
```

**In `ggplot2`**

```{r fig.cap = "Linear regression between \\texttt{eruptions} and \\texttt{wating} from the model applied to testing dataset, with 95% prediction intervals, using \\texttt{ggplot2}.", fig.width = 4, fig.height = 4, results = "Hold"}

pred1 <- data.frame(predict(lm1,
                 newdata = testFaith,
                 interval = "prediction",
                 level = 0.95))

ggplot(testFaith, aes(x = waiting, y = eruptions)) +
  geom_line(aes(y = predict(lm1, newdata = testFaith)),
            color = "red") +
  geom_line(aes(y = pred1$lwr),
            color = "red",
            linetype = "dashed") +
  geom_line(aes(y = pred1$upr),
            color = "red",
            linetype = "dashed") +
  geom_point(alpha = 0.3,
             color = "red") +
  labs(x = "Waiting Time (m)", y = " Eruption Duration (m)") +
  theme_pubclean()
```

## Same process with `caret`

```{r}
modFit <- train(eruptions ~ waiting,
                data = trainFaith,
                method = "lm")

summary(modFit$finalModel)
```

### Comparing predicted to actual values

```{r fig.cap = "Linear regression between \\texttt{eruptions} and \\texttt{wating} from the model applied to testing dataset, with 95% prediction intervals, using \\texttt{ggplot2}.", fig.width = 4, fig.height = 4, results = "Hold"}

ggplot(testFaith, aes(x = eruptions, y = predict(lm1, newdata = testFaith))) +
  geom_smooth(method = "lm",
              color = "blue") +
  geom_point(alpha = 0.3,
             color = "blue") +
  labs(x = "Actual Eruption Duration (m)", 
       y = "Predicted Eruption Duration (m)") +
  stat_cor(aes(label = paste(..rr.label..,
                             cut(..p..,
                                 breaks = c(-Inf, 
                                            0.0001, 
                                            0.001, 
                                            0.01, 
                                            0.05, 
                                            Inf),
                                 labels = c("'****'", 
                                            "'***'", 
                                            "'**'", 
                                            "'*'", 
                                            "''")),
                             sep = "~")),
           label.y.npc = 0.9,
           color = "black") +
  theme_pubclean()
```

## Notes and further reading

-   Regression models with multiple covariates can be included
-   Often useful in combination with other models
-   [Elements of statistical learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/) [@hastieElementsStatisticalLearning2009]
-   [Modern applied statistics with S](http://www.amazon.com/Modern-Applied-Statistics-W-N-Venables/dp/0387954570) [@venablesModernAppliedStatistics2002]
-   [Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/) [@jamesIntroductionStatisticalLearning2013]

------------------------------------------------------------------------

# **Lecture 9**: Predicting with Regression Multiple Covariates

This lecture's about two important points:

1.  Predicting with regression and using **multiple covariates**

2.  More importantly it's about exploring a data set and trying to **identify which predictors are the most important** to include in our prediction model

## Example: predicting wages

We will again use the `Wage` dataset, from the `ISLR` package, from the book *Introduction to Statistical Learning* [@jamesIntroductionStatisticalLearning2013].

Here, we subset the variable excluding `logwage`, which is the variable we are trying to predict.

```{r}
library(ISLR)
library(ggplot2)
library(caret)

data(Wage)

Wage <- subset(Wage,select=-c(logwage))
summary(Wage)
```

Some important aspect visible from the summary, are that the sample is only of males, and from the Middle Atlantic region.

### Get training/test sets

```{r}
inTrain <- createDataPartition(y=Wage$wage,
                              p=0.7, 
                              list=FALSE)

training <- Wage[inTrain,]
testing <- Wage[-inTrain,]

dim(training)
dim(testing)
```

### Feature plot

```{r fig.cap = "\\texttt{featurePlot} from the \\texttt{caret} package. In this case, we asked for the association of three predictors (\\texttt{age, education, jobclass}) with the dependent variable (\\texttt{wage}), as well as amongst them. In particular, you need to look at the first row (i.e. the assiociations between the predictors and the outcome variable); in this case, for example, there is a clear association between \\texttt{ajobclass} and \\texttt{wage}.", fig.width = 8, fig.height = 8, results = "Hold"}
featurePlot(x = training[,c("age", "education", "jobclass")],
            y = training$wage,
            plot = "pairs")

```

### Association between `age` and `wage`

As we saw before, the association between `age` and `wage` is curved, but also there is an important group of outliers, mainly predicted by `jobclass`.

```{r fig.cap = "\\textbf{A.} Association between wage and age. \\textbf{B.}  Association between wage and age by job class. \\textbf{C.}  Association between wage and age by education level.", fig.width = 8, fig.height = 8, results = "Hold"}

p1 <- ggplot(training, aes(x = age, y = wage)) +
  geom_point(alpha = 0.3) +
  labs(x = "Age", y = "Wage") +
  theme_pubclean()

p2 <- ggplot(training, aes(x = age, y = wage, colour = jobclass)) +
  geom_point(alpha = 0.5) +
  labs(x = "Age", y = "Wage", colour = "Job class") +
  theme_pubclean()

p3 <- ggplot(training, aes(x = age, y = wage, colour = education)) +
  geom_point(alpha = 0.5) +
  labs(x = "Age", y = "Wage", colour = "Education") +
  theme_pubclean()
  
ggarrange(p1, ggarrange(p2, p3,
                        ncol = 2,
                        labels = c("", "C"),
                        align = "h"),
          labels = c("A", "B"),
          nrow = 2)
```

## Fit a linear model

$$ED_{i} = b_{0} + b_{1}age + b_{2}I(Jobclass_{i} = "Information") + \sum_{k=1}^4 \gamma_k I(education_i= level k)$$

-   Here, the idea is that we have some intercept terms ($b_{0}$), so that's just the baseline level of wage that we might have

-   Then, we might have a relationship with the age of the person ($b_{1}age$)

-   Then we might have relationship with what job class you're in. So one way that we typically do that is, by fitting an indicator variable

    -   An indicator variable is a variable that's denoted like $I$ in mathematical notation. It just says, if the job class for the $i^{th}$ person is equal to $Information$, this variable's equal to 1. If the job class for the $i^{th}$ person is [not]{.ul} equal to information, then this information is equal to 0

    -   This represents the difference in the wages between the people with job class equal to $Information$ versus job class equal to not information, when you fix all the other variables in the regression model

-   You can also do this for $Education$. This is a little bit more complicated because there are multiple education levels

    -   We create an indicator variable for each of the different $Education$

    -   And so the variable's equal to 1, if the education for person $i^{th}$ is equal to level $K$, that variables equal to 1, or otherwise 0

```{r}
modFit <- train(wage ~ age + jobclass + education,
                method = "lm",
                data = training)

finMod <- modFit$finalModel
modFit
```

## Diagnostics: Homoscedasticity

Residuals vs fitted values. The line would be centred at zero because the residuals is the difference between the model prediction and the actual real values that we're trying to predict.

**Base R**

```{r fig.cap = "Homoscedasticity of the model with base R. In addition to the red line (which should be around 0), this plor highlights outliers.", results = "Hold"}
plot(finMod,1,pch=19,cex=0.5,col="#00000010")
```

In this case, there is still a couple of outliers up here that have been labelled for you in the plot created with Base R.

**Performance**

```{r fig.cap = "Homoscedasticity of the model with \\texttt{performance}.", results = "Hold"}

library(performance)

check_model(finMod,
            check = "ncv")
```

Those variables might be variables that we want to try to explore a little bit further and see if we can identify any other predictors in our data set that might be able to explain them.

### Colour by variables not included in the model

For example, plotting by race, shows how some of the outliers may be explained by race.

```{r fig.cap = "Association between wage and age by job race.", results = "Hold"}
ggplot(training, aes(x = finMod$fitted.values, y = finMod$residuals)) +
  geom_point(alpha = 0.5, aes(colour = race)) +
  geom_smooth(method = "loess", colour = "black", size = 0.5, se = FALSE) +
  labs(x = "Fitted values", y = "Residuals", colour = "Race") +
  theme_pubclean()
```

### Plot by index

For example, plotting by race, shows how some of the outliers may be explained by race.

```{r fig.height = 2.4, fig.align = "center", fig.cap = "Index vs residuals. \\textbf{A.} Plot produced in the lecture (I could not reproduce it). \\textbf{B.} Plot created with \\texttt{ggplot2}.", results = "Hold"}
library(png)

img1 <- readPNG("indexresiduals.png")

im_A <- ggplot() + background_image(img1)
im_B <- ggplot(training, aes(x = c(1:length(finMod$residuals)), 
                             y = finMod$residuals)) +
  geom_point(alpha = 0.5, aes(colour = race)) +
  geom_smooth(method = "loess", colour = "black", size = 0.5, se = FALSE) +
  labs(x = "Index", y = "Residuals", colour = "Race") +
  theme_pubclean()

ggarrange(im_A, im_B,
          ncol = 2,
          widths = c(1.06, 1),
          labels = "AUTO")
```

In panel **A**, high residuals seem to be happening at the right end of the highest row numbers.

There is a trend with respect to row numbers and so whenever you can see a trend or a outlier like that with respect to the row numbers, it suggests that there is a variable missing from your model because there should not be any relationship to the order in which the variables appear in the data set.

When you see a trend like this, or outliers like this, at one end of this plot, there's a relationship with respect to time, or age, or some other continuous variable that the rows are ordered by.

## Predicted versus truth in the test set

To understand the model, plotting actual vs predicted values in the test set is a good option. Here, it is done by year, trying to understand where the outliers come from.

```{r fig.cap = "Actual versus predicted Wage values in the test set.", results = "Hold"}
pred <- predict(modFit, testing)

ggplot(testing, aes(x = wage, y = pred, colour = year)) +
  geom_point(alpha = 0.5) +
  labs(x = "Wage (Actual)", y = "Wage (Predicted)", colour = "Year") +
  theme_pubclean() +
  theme(legend.position = "right")
```

Something to keep in mind is that if you do this sort of exploration in the test set, you cannot then go back and re-update your model in the training set because that would be using the test set to rebuild your predictors.

This is more like a post-mortem on your analysis or a way to try to determine whether your analysis worked or not.

## If you want to use all covariates

```{r warining = FALSE, message = FALSE, fig.cap = "Actual versus predicted Wage values in the test set, from a model using all covariates.", results = "Hold"}
modFitAll <- train(wage ~ .,
                   data = training,
                   method = "lm")

pred <- predict(modFitAll, testing)

ggplot(testing, aes(x = wage, y = pred)) +
  geom_point(alpha = 0.5) +
  labs(x = "Wage (Actual)", y = "Wage (Predicted)") +
  theme_pubclean()
```

## Notes and further reading

-   **Linear regression** is often useful in combination with other models
-   **Exploratory analysis** (like the plots we made) are often very useful in helping identify predictors
-   [Elements of statistical learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/) [@hastieElementsStatisticalLearning2009]
-   [Modern applied statistics with S](http://www.amazon.com/Modern-Applied-Statistics-W-N-Venables/dp/0387954570) [@venablesModernAppliedStatistics2002]
-   [Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/) [@jamesIntroductionStatisticalLearning2013]

------------------------------------------------------------------------

# **References**
